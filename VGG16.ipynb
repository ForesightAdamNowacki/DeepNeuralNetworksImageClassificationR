{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & GPU connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os, shutil\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import math\n",
    "from keras import callbacks\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:01:01.377614 EOSINOPHIL train dataset created successfully : number of observations = 300\n",
      "2019-11-17 22:01:01.453411 EOSINOPHIL validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:01.530203 EOSINOPHIL test dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:02.115614 LYMPHOCYTE train dataset created successfully : number of observations = 300\n",
      "2019-11-17 22:01:02.193406 LYMPHOCYTE validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:02.274189 LYMPHOCYTE test dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:02.855634 MONOCYTE train dataset created successfully : number of observations = 300\n",
      "2019-11-17 22:01:02.932429 MONOCYTE validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:03.009223 MONOCYTE test dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:03.586679 NEUTROPHIL train dataset created successfully : number of observations = 300\n",
      "2019-11-17 22:01:03.664471 NEUTROPHIL validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 22:01:03.742263 NEUTROPHIL test dataset created successfully : number of observations = 100\n"
     ]
    }
   ],
   "source": [
    "# Path to original data:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells'\n",
    "\n",
    "# Path to new data store:\n",
    "base_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Create train, validation, test folders:\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'Validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Train: \n",
    "train_eosinophil_dir = os.path.join(train_dir, 'EOSINOPHIL')\n",
    "os.mkdir(train_eosinophil_dir)\n",
    "train_lymphocyte_dir = os.path.join(train_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(train_lymphocyte_dir)\n",
    "train_monocyte_dir = os.path.join(train_dir, 'MONOCYTE')\n",
    "os.mkdir(train_monocyte_dir)\n",
    "train_neutrophil_dir = os.path.join(train_dir, 'NEUTROPHIL')\n",
    "os.mkdir(train_neutrophil_dir)\n",
    "\n",
    "# Validation:\n",
    "validation_eosinophil_dir = os.path.join(validation_dir, 'EOSINOPHIL')\n",
    "os.mkdir(validation_eosinophil_dir)\n",
    "validation_lymphocyte_dir = os.path.join(validation_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(validation_lymphocyte_dir)\n",
    "validation_monocyte_dir = os.path.join(validation_dir, 'MONOCYTE')\n",
    "os.mkdir(validation_monocyte_dir)\n",
    "validation_neutrophil_dir = os.path.join(validation_dir, 'NEUTROPHIL')\n",
    "os.mkdir(validation_neutrophil_dir)\n",
    "\n",
    "# Test:\n",
    "test_eosinophil_dir = os.path.join(test_dir, 'EOSINOPHIL')\n",
    "os.mkdir(test_eosinophil_dir)\n",
    "test_lymphocyte_dir = os.path.join(test_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(test_lymphocyte_dir)\n",
    "test_monocyte_dir = os.path.join(test_dir, 'MONOCYTE')\n",
    "os.mkdir(test_monocyte_dir)\n",
    "test_neutrophil_dir = os.path.join(test_dir, 'NEUTROPHIL')\n",
    "os.mkdir(test_neutrophil_dir)\n",
    "\n",
    "# Raname files to more structured:\n",
    "def change_files_names(path, class_name): \n",
    "    i = 0\n",
    "    for filename in os.listdir(path):        \n",
    "        dst = str(i) + \"_\" + class_name + \".jpg\"\n",
    "        src = path + \"\\\\\" + filename \n",
    "        dst = path + \"\\\\\" + dst \n",
    "        os.rename(src, dst) \n",
    "        i += 1\n",
    "        \n",
    "# Train, Validation, Test Sizes:\n",
    "train_size = 300\n",
    "validation_size = train_size + 100\n",
    "test_size = validation_size + 100\n",
    "\n",
    "# EOSINOPHIL:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\EOSINOPHIL'\n",
    "change_files_names(original_dataset_dir, \"Eosinophil\")\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL train dataset created successfully\", \": number of observations =\", len(os.listdir(train_eosinophil_dir)))\n",
    "\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_eosinophil_dir)))\n",
    "\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL test dataset created successfully\", \": number of observations =\", len(os.listdir(test_eosinophil_dir)))\n",
    "    \n",
    "# LYMPHOCYTE:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\LYMPHOCYTE'\n",
    "change_files_names(original_dataset_dir, \"Lymphocyte\")\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE train dataset created successfully\", \": number of observations =\", len(os.listdir(train_lymphocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_lymphocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE test dataset created successfully\", \": number of observations =\", len(os.listdir(test_lymphocyte_dir)))\n",
    "    \n",
    "# MONOCYTE:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\MONOCYTE'\n",
    "change_files_names(original_dataset_dir, \"Monocyte\")\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE train dataset created successfully\", \": number of observations =\", len(os.listdir(train_monocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_monocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE test dataset created successfully\", \": number of observations =\", len(os.listdir(test_monocyte_dir)))\n",
    "    \n",
    "# NEUTROPHIL:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\NEUTROPHIL'\n",
    "change_files_names(original_dataset_dir, \"Neutrophil\")\n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL train dataset created successfully\", \": number of observations =\", len(os.listdir(train_neutrophil_dir)))\n",
    "\n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_neutrophil_dir)))\n",
    "    \n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL test dataset created successfully\", \": number of observations =\", len(os.listdir(test_neutrophil_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 architecture & generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 75, 75, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 75, 75, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 15,797,824\n",
      "Trainable params: 15,788,864\n",
      "Non-trainable params: 8,960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 1200 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Parameters:\n",
    "train_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Train\"\n",
    "validation_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Validation\"\n",
    "test_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Test\"\n",
    "\n",
    "input_shape = 150 # originally 224\n",
    "canales = 3\n",
    "batch_size = 8\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "epochs = 10\n",
    "optimizer = \"adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "class_mode = \"categorical\"\n",
    "patience = 5\n",
    "\n",
    "def count_observations(directory):\n",
    "    i = 0\n",
    "    files = 0\n",
    "    for folder in os.listdir(directory):\n",
    "        directory_2 = directory + \"\\\\\" + folder\n",
    "        count = sum([len(files) for r, d, files in os.walk(directory_2)])\n",
    "        files = files + count\n",
    "    return(files)\n",
    "\n",
    "train_images_count = count_observations(train_dir)  \n",
    "validation_images_count = count_observations(validation_dir)\n",
    "test_images_count = count_observations(test_dir)\n",
    "train_steps_per_epoch = math.ceil(train_images_count/batch_size)\n",
    "validation_steps_per_epoch = math.ceil(validation_images_count/batch_size)\n",
    "test_steps_per_epoch = math.ceil(test_images_count/batch_size)\n",
    "\n",
    "##########\n",
    "# VGG16 architecture:\n",
    "VGG16 = models.Sequential()\n",
    "\n",
    "# Conv 1-2:\n",
    "VGG16.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\",\n",
    "                        input_shape = (input_shape, input_shape, canales)))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 3-4:\n",
    "VGG16.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 5-7:\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 8-10:\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 11-13:\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Dense: 14-15\n",
    "VGG16.add(layers.Flatten())\n",
    "VGG16.add(layers.Dense(units = 128, activation = \"linear\")) # originally 4096\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Dropout(rate = 0.5))\n",
    "VGG16.add(layers.Dense(units = 128, activation = \"linear\")) # originally 4096\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Dropout(rate = 0.5))\n",
    "print(VGG16.summary())\n",
    "\n",
    "# Dense: 16\n",
    "VGG16.add(layers.Dense(units = num_classes, activation = \"softmax\"))\n",
    "\n",
    "##########\n",
    "# Model compilation:\n",
    "VGG16.compile(optimizer = optimizer,\n",
    "              loss = loss,\n",
    "              metrics = metrics)\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\")\n",
    "# os.mkdir(os.path.join(os.getcwd(), 'Models'))\n",
    "os.chdir(\"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\Models\")\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor = \"val_accuracy\",\n",
    "        patience = patience), \n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath = os.path.join(os.getcwd(), \"VGG16-weights-improvement-{epoch:02d}-{val_accuracy:.4f}.hdf5\"),\n",
    "        monitor = 'val_accuracy',\n",
    "        verbose = 1, \n",
    "        save_best_only = True,\n",
    "        mode = 'max')]\n",
    "\n",
    "##########\n",
    "# Generators:\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 45, \n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1, \n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = True,\n",
    "    fill_mode = \"nearest\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = train_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 45, \n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1, \n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = True,\n",
    "    fill_mode = \"nearest\")\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory = validation_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = test_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)\n",
    "\n",
    "# tensorboard --logdir = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 21s 140ms/step - loss: 1.9095 - accuracy: 0.2283 - val_loss: 1.4157 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25000, saving model to C:\\Users\\admin\\Desktop\\GitHub\\DNN\\Datasets\\Blood_cells\\Models\\VGG16-weights-improvement-01-0.2500.hdf5\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6538 - accuracy: 0.2633 - val_loss: 1.3994 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.25000\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 1.6925 - accuracy: 0.2342 - val_loss: 1.3537 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.25000\n",
      "Epoch 4/10\n",
      " 72/150 [=============>................] - ETA: 7s - loss: 1.5815 - accuracy: 0.2812"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Fit model:\n",
    "history = VGG16.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_steps_per_epoch,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps_per_epoch, \n",
    "    callbacks = callbacks_list, \n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [1.3819966316223145,\n",
       "  1.387467622756958,\n",
       "  1.3516263961791992,\n",
       "  1.4068050384521484,\n",
       "  1.4071420431137085,\n",
       "  1.3838331699371338],\n",
       " 'val_accuracy': [0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       " 'loss': [1.7633855708440145,\n",
       "  1.6389277410507201,\n",
       "  1.623055607477824,\n",
       "  1.5981233342488608,\n",
       "  1.5651938501993816,\n",
       "  1.517081470489502],\n",
       " 'accuracy': [0.24166666, 0.25083333, 0.2275, 0.23833333, 0.24, 0.26083332]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Train loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Train & validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
