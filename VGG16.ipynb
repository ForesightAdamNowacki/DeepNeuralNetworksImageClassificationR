{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & GPU connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os, shutil\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import math\n",
    "from keras import callbacks\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 18:45:35.577906 EOSINOPHIL train dataset created successfully : number of observations = 300\n",
      "2019-11-17 18:45:35.654700 EOSINOPHIL validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:35.733490 EOSINOPHIL test dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:36.317926 LYMPHOCYTE train dataset created successfully : number of observations = 300\n",
      "2019-11-17 18:45:36.394721 LYMPHOCYTE validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:36.473510 LYMPHOCYTE test dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:37.040992 MONOCYTE train dataset created successfully : number of observations = 300\n",
      "2019-11-17 18:45:37.116790 MONOCYTE validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:37.189595 MONOCYTE test dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:37.748101 NEUTROPHIL train dataset created successfully : number of observations = 300\n",
      "2019-11-17 18:45:37.835867 NEUTROPHIL validation dataset created successfully : number of observations = 100\n",
      "2019-11-17 18:45:37.911665 NEUTROPHIL test dataset created successfully : number of observations = 100\n"
     ]
    }
   ],
   "source": [
    "# Path to original data:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells'\n",
    "\n",
    "# Path to new data store:\n",
    "base_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Create train, validation, test folders:\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'Validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Train: \n",
    "train_eosinophil_dir = os.path.join(train_dir, 'EOSINOPHIL')\n",
    "os.mkdir(train_eosinophil_dir)\n",
    "train_lymphocyte_dir = os.path.join(train_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(train_lymphocyte_dir)\n",
    "train_monocyte_dir = os.path.join(train_dir, 'MONOCYTE')\n",
    "os.mkdir(train_monocyte_dir)\n",
    "train_neutrophil_dir = os.path.join(train_dir, 'NEUTROPHIL')\n",
    "os.mkdir(train_neutrophil_dir)\n",
    "\n",
    "# Validation:\n",
    "validation_eosinophil_dir = os.path.join(validation_dir, 'EOSINOPHIL')\n",
    "os.mkdir(validation_eosinophil_dir)\n",
    "validation_lymphocyte_dir = os.path.join(validation_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(validation_lymphocyte_dir)\n",
    "validation_monocyte_dir = os.path.join(validation_dir, 'MONOCYTE')\n",
    "os.mkdir(validation_monocyte_dir)\n",
    "validation_neutrophil_dir = os.path.join(validation_dir, 'NEUTROPHIL')\n",
    "os.mkdir(validation_neutrophil_dir)\n",
    "\n",
    "# Test:\n",
    "test_eosinophil_dir = os.path.join(test_dir, 'EOSINOPHIL')\n",
    "os.mkdir(test_eosinophil_dir)\n",
    "test_lymphocyte_dir = os.path.join(test_dir, 'LYMPHOCYTE')\n",
    "os.mkdir(test_lymphocyte_dir)\n",
    "test_monocyte_dir = os.path.join(test_dir, 'MONOCYTE')\n",
    "os.mkdir(test_monocyte_dir)\n",
    "test_neutrophil_dir = os.path.join(test_dir, 'NEUTROPHIL')\n",
    "os.mkdir(test_neutrophil_dir)\n",
    "\n",
    "# Raname files to more structured:\n",
    "def change_files_names(path, class_name): \n",
    "    i = 0\n",
    "    for filename in os.listdir(path):        \n",
    "        dst = str(i) + \"_\" + class_name + \".jpg\"\n",
    "        src = path + \"\\\\\" + filename \n",
    "        dst = path + \"\\\\\" + dst \n",
    "        os.rename(src, dst) \n",
    "        i += 1\n",
    "        \n",
    "# Train, Validation, Test Sizes:\n",
    "train_size = 300\n",
    "validation_size = train_size + 100\n",
    "test_size = validation_size + 100\n",
    "\n",
    "# EOSINOPHIL:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\EOSINOPHIL'\n",
    "change_files_names(original_dataset_dir, \"Eosinophil\")\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL train dataset created successfully\", \": number of observations =\", len(os.listdir(train_eosinophil_dir)))\n",
    "\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_eosinophil_dir)))\n",
    "\n",
    "fnames = ['{}_Eosinophil.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_eosinophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"EOSINOPHIL test dataset created successfully\", \": number of observations =\", len(os.listdir(test_eosinophil_dir)))\n",
    "    \n",
    "# LYMPHOCYTE:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\LYMPHOCYTE'\n",
    "change_files_names(original_dataset_dir, \"Lymphocyte\")\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE train dataset created successfully\", \": number of observations =\", len(os.listdir(train_lymphocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_lymphocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Lymphocyte.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_lymphocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"LYMPHOCYTE test dataset created successfully\", \": number of observations =\", len(os.listdir(test_lymphocyte_dir)))\n",
    "    \n",
    "# MONOCYTE:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\MONOCYTE'\n",
    "change_files_names(original_dataset_dir, \"Monocyte\")\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE train dataset created successfully\", \": number of observations =\", len(os.listdir(train_monocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_monocyte_dir)))\n",
    "\n",
    "fnames = ['{}_Monocyte.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_monocyte_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"MONOCYTE test dataset created successfully\", \": number of observations =\", len(os.listdir(test_monocyte_dir)))\n",
    "    \n",
    "# NEUTROPHIL:\n",
    "original_dataset_dir = 'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\NEUTROPHIL'\n",
    "change_files_names(original_dataset_dir, \"Neutrophil\")\n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(train_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL train dataset created successfully\", \": number of observations =\", len(os.listdir(train_neutrophil_dir)))\n",
    "\n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(train_size, validation_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL validation dataset created successfully\", \": number of observations =\", len(os.listdir(validation_neutrophil_dir)))\n",
    "    \n",
    "fnames = ['{}_Neutrophil.jpg'.format(i) for i in range(validation_size, test_size)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_neutrophil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "print(datetime.datetime.now(), \"NEUTROPHIL test dataset created successfully\", \": number of observations =\", len(os.listdir(test_neutrophil_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 architecture & generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 75, 75, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 75, 75, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 15,797,824\n",
      "Trainable params: 15,788,864\n",
      "Non-trainable params: 8,960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 1200 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n",
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Parameters:\n",
    "train_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Train\"\n",
    "validation_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Validation\"\n",
    "test_dir = \"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\DATA\\\\Test\"\n",
    "\n",
    "input_shape = 150 # originally 224\n",
    "canales = 3\n",
    "batch_size = 8\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "epochs = 10\n",
    "optimizer = \"adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "class_mode = \"categorical\"\n",
    "\n",
    "def count_observations(directory):\n",
    "    i = 0\n",
    "    files = 0\n",
    "    for folder in os.listdir(directory):\n",
    "        directory_2 = directory + \"\\\\\" + folder\n",
    "        count = sum([len(files) for r, d, files in os.walk(directory_2)])\n",
    "        files = files + count\n",
    "    return(files)\n",
    "\n",
    "train_images_count = count_observations(train_dir)  \n",
    "validation_images_count = count_observations(validation_dir)\n",
    "test_images_count = count_observations(test_dir)\n",
    "train_steps_per_epoch = math.ceil(train_images_count/batch_size)\n",
    "validation_steps_per_epoch = math.ceil(validation_images_count/batch_size)\n",
    "test_steps_per_epoch = math.ceil(test_images_count/batch_size)\n",
    "\n",
    "##########\n",
    "# VGG16 architecture:\n",
    "VGG16 = models.Sequential()\n",
    "\n",
    "# Conv 1-2:\n",
    "VGG16.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\",\n",
    "                        input_shape = (input_shape, input_shape, canales)))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 3-4:\n",
    "VGG16.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 5-7:\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 8-10:\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Conv 11-13:\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), activation = \"linear\", padding = \"same\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.MaxPool2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\"))\n",
    "\n",
    "# Dense: 14-15\n",
    "VGG16.add(layers.Flatten())\n",
    "VGG16.add(layers.Dense(units = 128, activation = \"linear\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Dropout(rate = 0.5))\n",
    "VGG16.add(layers.Dense(units = 128, activation = \"linear\"))\n",
    "VGG16.add(layers.BatchNormalization())\n",
    "VGG16.add(layers.Activation(\"relu\"))\n",
    "VGG16.add(layers.Dropout(rate = 0.5))\n",
    "print(VGG16.summary())\n",
    "\n",
    "# Dense: 16\n",
    "VGG16.add(layers.Dense(units = num_classes, activation = \"softmax\"))\n",
    "\n",
    "##########\n",
    "# Model compilation:\n",
    "VGG16.compile(optimizer = optimizer,\n",
    "              loss = loss,\n",
    "              metrics = metrics)\n",
    "\n",
    "#os.chdir(\"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\")\n",
    "#os.mkdir(os.path.join(os.getcwd(), 'Models'))\n",
    "#os.chdir(os.path.join(os.getcwd(), 'Models'))\n",
    "os.chdir(\"C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\Models\")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath = os.path.join(os.getcwd(), \"VGG16-weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"),\n",
    "    monitor = 'val_accuracy',\n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    mode = 'max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "##########\n",
    "# Generators:\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 45, \n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1, \n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = True,\n",
    "    fill_mode = \"nearest\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = train_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 45, \n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1, \n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = True,\n",
    "    fill_mode = \"nearest\")\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory = validation_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = test_dir, \n",
    "    target_size = (input_shape, input_shape), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = class_mode,\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 21s 142ms/step - loss: 1.7798 - accuracy: 0.2483 - val_loss: 1.4165 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25000, saving model to C:\\Users\\admin\\Desktop\\GitHub\\DNN\\Datasets\\Blood_cells\\Models\\VGG16-weights-improvement-01-0.25.hdf5\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6979 - accuracy: 0.2275 - val_loss: 1.4229 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.25000\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.6021 - accuracy: 0.2558 - val_loss: 1.4210 - val_accuracy: 0.2450\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.25000\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6009 - accuracy: 0.2408 - val_loss: 1.3388 - val_accuracy: 0.2475\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.25000\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.5118 - accuracy: 0.2483 - val_loss: 1.3595 - val_accuracy: 0.2250\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.25000\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.5026 - accuracy: 0.2500 - val_loss: 1.3907 - val_accuracy: 0.3075\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.25000 to 0.30750, saving model to C:\\Users\\admin\\Desktop\\GitHub\\DNN\\Datasets\\Blood_cells\\Models\\VGG16-weights-improvement-06-0.31.hdf5\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.4505 - accuracy: 0.2792 - val_loss: 1.1731 - val_accuracy: 0.3425\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.30750 to 0.34250, saving model to C:\\Users\\admin\\Desktop\\GitHub\\DNN\\Datasets\\Blood_cells\\Models\\VGG16-weights-improvement-07-0.34.hdf5\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.2117 - accuracy: 0.4183 - val_loss: 1.7734 - val_accuracy: 0.3725\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.34250 to 0.37250, saving model to C:\\Users\\admin\\Desktop\\GitHub\\DNN\\Datasets\\Blood_cells\\Models\\VGG16-weights-improvement-08-0.37.hdf5\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.0540 - accuracy: 0.4950 - val_loss: 1.3077 - val_accuracy: 0.2750\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.37250\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 0.9921 - accuracy: 0.5400 - val_loss: 2.0261 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.37250\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Fit model:\n",
    "history = VGG16.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch = train_steps_per_epoch,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps_per_epoch, \n",
    "    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\GitHub\\\\DNN\\\\Datasets\\\\Blood_cells\\\\Models'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
